{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73876c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0771a95",
   "metadata": {},
   "source": [
    "Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af6fb0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0415a2a882ac46a5b0fca67ffc8ee727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ae4089",
   "metadata": {},
   "source": [
    "Data Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f0beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(text, label=\"\"):\n",
    "    return f\"\"\"### Інструкція:\n",
    "Класифікуй текст як один із класів:\n",
    "Neutral або Sarcasm.\n",
    "\n",
    "### Текст:\n",
    "{text}\n",
    "\n",
    "### Відповідь:\n",
    "{label}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cdd47a",
   "metadata": {},
   "source": [
    "Evaluation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b965dc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    prompt = (\n",
    "        \"Завдання: Визнач тональність тексту. \"\n",
    "        \"Відповідь має бути одним словом: 'Sarcasm' або 'Neutral'.\\n\\n\"\n",
    "        f\"Текст: {text}\\n\"\n",
    "        \"Категорія:\"\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        **inputs, max_new_tokens=5, do_sample=False, pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    prediction_tokens = outputs[0][input_length:]\n",
    "    decoded = (\n",
    "        tokenizer.decode(prediction_tokens, skip_special_tokens=True).strip().lower()\n",
    "    )\n",
    "\n",
    "    # Map the word response to a binary integer\n",
    "    if \"sarcasm\" in decoded:\n",
    "        return 1\n",
    "    return 0  # Default to Neutral (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514a2a82",
   "metadata": {},
   "source": [
    "Result logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b71284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_result(stage, text, gold, pred, path=\"results.csv\"):\n",
    "    header = [\"stage\", \"text\", \"gold\", \"pred\"]\n",
    "\n",
    "    # Force gold and pred to integers to ensure consistency in the CSV\n",
    "    row = [stage, text, int(gold), int(pred)]\n",
    "\n",
    "    file_exists = os.path.isfile(path)\n",
    "\n",
    "    with open(path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists:\n",
    "            writer.writerow(header)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ceae67",
   "metadata": {},
   "source": [
    "Zero-shot baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e2ba80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\hate_speech_ukranian_llm\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст: Його вигнали з роботи тому що він розмовляв російською.\n",
      "Expected: 0 | Predicted: 0\n",
      "------------------------------\n",
      "Текст: Сьогодні так класно погуляли з друзями, і погода була супер.\n",
      "Expected: 0 | Predicted: 0\n",
      "------------------------------\n",
      "Текст: Розумні думки намагаються тебе наздогнати, але ти бистріший.\n",
      "Expected: 1 | Predicted: 0\n",
      "------------------------------\n",
      "Текст: Так, звичайно, повтори мені це ще 5 разів, а то я перший раз не почув)\n",
      "Expected: 1 | Predicted: 0\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    (\"Його вигнали з роботи тому що він розмовляв російською.\", 0),\n",
    "    (\"Сьогодні так класно погуляли з друзями, і погода була супер.\", 0),\n",
    "    (\"Розумні думки намагаються тебе наздогнати, але ти бистріший.\", 1),\n",
    "    (\n",
    "        \"Так, звичайно, повтори мені це ще 5 разів, а то я перший раз не почув)\",\n",
    "        1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "for text, gold in test_texts:\n",
    "    pred = predict(text)\n",
    "    log_result(\"zero-shot-ukrainian\", text, gold, pred)\n",
    "    print(f\"Текст: {text}\\nExpected: {gold} | Predicted: {pred}\\n\" + \"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c8377",
   "metadata": {},
   "source": [
    "Compute Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f664a547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Accuracy ---\n",
      "stage\n",
      "zero-shot    0.5\n",
      "dtype: float64\n",
      "\n",
      "Report for zero-shot:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 4, does not match size of target_names, 2. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m subset \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m stage]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mReport for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m---> 16\u001b[0m     \u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgold\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpred\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNeutral\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSarcasm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m )\n",
      "File \u001b[1;32mc:\\hate_speech_ukranian_llm\\.venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    216\u001b[0m         )\n\u001b[0;32m    217\u001b[0m     ):\n\u001b[1;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    228\u001b[0m     )\n",
      "File \u001b[1;32mc:\\hate_speech_ukranian_llm\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2970\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2964\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2965\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2966\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[0;32m   2967\u001b[0m             )\n\u001b[0;32m   2968\u001b[0m         )\n\u001b[0;32m   2969\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2970\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2971\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2972\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. Try specifying the labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2973\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[0;32m   2974\u001b[0m         )\n\u001b[0;32m   2975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2976\u001b[0m     target_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes, 4, does not match size of target_names, 2. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"results.csv\")\n",
    "\n",
    "# Print overall accuracy for each stage\n",
    "acc = df.groupby(\"stage\").apply(\n",
    "    lambda x: (x[\"gold\"] == x[\"pred\"]).mean(), include_groups=False\n",
    ")\n",
    "print(\"--- Accuracy ---\")\n",
    "print(acc)\n",
    "\n",
    "# Detailed report (Precision, Recall, F1)\n",
    "# Use target_names to keep it readable\n",
    "for stage in df[\"stage\"].unique():\n",
    "    subset = df[df[\"stage\"] == stage]\n",
    "    print(f\"\\nReport for {stage}:\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            subset[\"gold\"], subset[\"pred\"], target_names=[\"Neutral\", \"Sarcasm\"]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b30b0f",
   "metadata": {},
   "source": [
    "LoRA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aee01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf85012a",
   "metadata": {},
   "source": [
    "Training Arguments & Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a5f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    output_dir=\"./results\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    max_seq_length=512,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21e83c3",
   "metadata": {},
   "source": [
    "Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb36a6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
