{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73876c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0771a95",
   "metadata": {},
   "source": [
    "Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af6fb0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0415a2a882ac46a5b0fca67ffc8ee727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ae4089",
   "metadata": {},
   "source": [
    "Data Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f0beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(text, label=\"\"):\n",
    "    return f\"\"\"### Ð†Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ñ–Ñ:\n",
    "ÐšÐ»Ð°ÑÐ¸Ñ„Ñ–ÐºÑƒÐ¹ Ñ‚ÐµÐºÑÑ‚ ÑÐº Ð¾Ð´Ð¸Ð½ Ñ–Ð· ÐºÐ»Ð°ÑÑ–Ð²:\n",
    "Neutral, Hate-Speech, Ð°Ð±Ð¾ Sarcasm.\n",
    "\n",
    "### Ð¢ÐµÐºÑÑ‚:\n",
    "{text}\n",
    "\n",
    "### Ð’Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ:\n",
    "{label}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cdd47a",
   "metadata": {},
   "source": [
    "Evaluation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b965dc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    # We add a \"System\" style instruction to tell the model to look for sarcasm\n",
    "    prompt = (\n",
    "        \"ÐšÐ»Ð°ÑÐ¸Ñ„Ñ–ÐºÑƒÐ¹ Ñ‚Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ–ÑÑ‚ÑŒ Ð¿Ð¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½Ð½Ñ. \"\n",
    "        \"ÐžÐ±ÐµÑ€Ð¸ Ð¾Ð´Ð½Ñƒ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ñ–ÑŽ: Hate-Speech, Sarcasm, Ð°Ð±Ð¾ Neutral.\\n\\n\"\n",
    "        f\"Ð¢ÐµÐºÑÑ‚: {text}\\n\"\n",
    "        \"ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ñ–Ñ:\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Use 'do_sample=False' to get the most likely answer (greedy decoding)\n",
    "    outputs = model.generate(\n",
    "        **inputs, max_new_tokens=5, do_sample=False, pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # We decode only the NEW tokens generated, not the whole prompt\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    prediction_tokens = outputs[0][input_length:]\n",
    "    decoded = tokenizer.decode(prediction_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Check the output against our labels\n",
    "    for label in [\"Hate-Speech\", \"Sarcasm\", \"Neutral\"]:\n",
    "        if label.lower() in decoded.lower():\n",
    "            return label\n",
    "    return \"Neutral\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514a2a82",
   "metadata": {},
   "source": [
    "Result logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8b71284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_result(stage, text, gold, pred, path=\"results.csv\"):\n",
    "    header = [\"stage\", \"text\", \"gold\", \"pred\"]\n",
    "    row = [stage, text, gold, pred]\n",
    "\n",
    "    file_exists = False\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            file_exists = True\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    with open(path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists:\n",
    "            writer.writerow(header)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ceae67",
   "metadata": {},
   "source": [
    "Zero-shot baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e2ba80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "c:\\hate_speech_ukranian_llm\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ð¢Ð¸ Ð¿Ð¾Ð²Ð½Ð¸Ð¹ Ð´ÑƒÑ€ÐµÐ½ÑŒ! Hate-Speech Hate-Speech\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ð“Ð°Ñ€Ð½Ðµ Ð²Ñ–Ð´ÐµÐ¾! Neutral Neutral\n",
      "ÐžÐ³Ð¾, ÑÐºÐ° Ð³Ð»Ð¸Ð±Ð¾ÐºÐ° Ð´ÑƒÐ¼ÐºÐ° ðŸ¤¡ Sarcasm Neutral\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    (\"Ð¢Ð¸ Ð¿Ð¾Ð²Ð½Ð¸Ð¹ Ð´ÑƒÑ€ÐµÐ½ÑŒ!\", \"Hate-Speech\"),\n",
    "    (\"Ð“Ð°Ñ€Ð½Ðµ Ð²Ñ–Ð´ÐµÐ¾!\", \"Neutral\"),\n",
    "    (\"ÐžÐ³Ð¾, ÑÐºÐ° Ð³Ð»Ð¸Ð±Ð¾ÐºÐ° Ð´ÑƒÐ¼ÐºÐ° ðŸ¤¡\", \"Sarcasm\"),\n",
    "]\n",
    "\n",
    "for text, gold in test_texts:\n",
    "    pred = predict(text)\n",
    "    log_result(\"zero-shot\", text, gold, pred)\n",
    "    print(f\"Ð¢ÐµÐºÑÑ‚: {text}\")\n",
    "    print(f\"Expected: {gold} | Predicted: {pred}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c8377",
   "metadata": {},
   "source": [
    "Compute Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f664a547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage\n",
      "zero-shot    0.666667\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avror\\AppData\\Local\\Temp\\ipykernel_1104\\3867451120.py:2: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  acc = df.groupby(\"stage\").apply(lambda x: (x.gold == x.pred).mean())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your results\n",
    "df = pd.read_csv(\"results.csv\")\n",
    "\n",
    "# Calculate accuracy: (True/False).mean() gives you the percentage of True\n",
    "accuracy = df.groupby(\"stage\").apply(\n",
    "    lambda x: (x[\"gold\"] == x[\"pred\"]).mean(), include_groups=False\n",
    ")\n",
    "\n",
    "print(\"--- Accuracy by Stage ---\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b30b0f",
   "metadata": {},
   "source": [
    "LoRA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aee01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf85012a",
   "metadata": {},
   "source": [
    "Training Arguments & Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a5f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    output_dir=\"./results\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    max_seq_length=512,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21e83c3",
   "metadata": {},
   "source": [
    "Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb36a6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
